# User Guide

Welcome to the LLM Connector Hub User Guide! This comprehensive guide covers all features and capabilities of the framework.

## Table of Contents

### Core Concepts

- **[Configuration](./configuration.md)** - Complete configuration reference for all components
- **[Providers](./providers.md)** - Provider-specific guides for OpenAI, Anthropic, and Google
- **[Middleware](./middleware.md)** - Middleware configuration, usage, and custom implementation

### Features

- **[Caching](./caching.md)** - Caching strategies and configuration for improved performance
- **[Error Handling](./error-handling.md)** - Error handling best practices and patterns
- **[Streaming](./streaming.md)** - Streaming responses guide and advanced patterns
- **[Health Monitoring](./health-monitoring.md)** - Health checks and monitoring setup

## Quick Navigation

### Getting Started
If you're new to LLM Connector Hub, start with the [Getting Started Guide](../getting-started.md).

### Common Tasks

- **Set up your first provider**: [Providers Guide](./providers.md)
- **Add retry logic**: [Middleware Guide](./middleware.md#retry-middleware)
- **Implement caching**: [Caching Guide](./caching.md)
- **Handle errors properly**: [Error Handling Guide](./error-handling.md)
- **Stream responses**: [Streaming Guide](./streaming.md)
- **Monitor system health**: [Health Monitoring Guide](./health-monitoring.md)

### API Reference
For detailed API documentation, see the [API Reference](../api/README.md).

### Deployment
For production deployment guides, see the [Deployment Guide](../deployment/README.md).

## Overview

LLM Connector Hub is designed to provide a unified, type-safe interface for working with multiple Large Language Model providers. This user guide will help you:

- Configure providers and middleware
- Implement advanced features like caching and streaming
- Handle errors gracefully
- Monitor system health
- Optimize performance

Each section includes practical examples and best practices to help you get the most out of the framework.
